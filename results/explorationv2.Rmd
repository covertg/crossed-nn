  ---
title: "exploration"
header-includes: |
  \makeatletter
  \def\verbatim@nolig@list{}
  \makeatother
  \usepackage{cancel}
  \usepackage[fontsize=9.5pt]{scrextend}
output:
  html_document:
    df_print: paged
  pdf_document:
    highlight: default
    latex_engine: xelatex
    number_sections: yes
classoption: landscape
monofont: Fira Code Retina
geometry: margin=0.5in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(Hmisc)
library(tidyverse)
library(boot)
set.seed(42)
options(boot.parallel='multicore')
options(boot.ncpus=8)
```

# Data wrangling

```{r}
forms <- 1:7
connective_contrasts <- c(", but then, ", "; but then, ", ", on the other hand, ", "; on the other hand, ",
                          "; by contrast, ", ", by contrast, ")
connective_summaries <- c(", thus, ", "; thus, ", ", that is, ", "; that is, ", ", therefore, ", "; therefore, ",
                          ", in short, ", "; in short, ", ", again, ", "; again, ", ", I repeat, ", "; I repeat, ")

results2tbl <- function(prefix, n) {
  # Bind all n results tsv's into one tbl, add experiment number column
  for (i in n) {
    input <- read.csv(paste(prefix, i, '.tsv', sep=''), sep='\t') %>%
      mutate(Experiment=i)
    if (i == 1) results <- tbl_df(input)
    else results <- bind_rows(results, input)
  }
  # Convert string T/F to boolean
  results <- results %>% mutate(correct=(correct=="True")) %>%
    mutate(crossing=(crossing=="True"))
  # Include column for connective type
  results <- results %>%
    mutate(Conn.Type=ifelse(X.Conn. %in% connective_summaries, "Summary", ifelse(X.Conn. %in% connective_contrasts, "Contrast", "ERR")))
  # Capitalize
  names(results) <- capitalize(names(results))
  return(results)
}
```

```{r}
lm1b <- results2tbl('lm1b_', forms)
gpt2 <- results2tbl('gpt2_', forms)

# Just a taste...
print(sample_n(lm1b, 10))
print(sample_n(gpt2, 10))
```

# Does NN know enough about SVO dependencies to be less surprised at summary than contrast?

<!--
We could use a t-test as follows, but since many datapoints come from the same premises, and many more datapoints come from similar overall sentences, arguably they are not independent samples.

A paired t-test could be helpful, but in this case we're looking at [Matching/Summary] - [Matching/Contrast] and there's no obvious 1-1 relation between the summary and contrasting connectives, so we don't have equal sample sizes. Below (t-tests with unequal variance) is kept for posterity, but instead we can do permutation tests to approximate null distributions.

However, moving away from hypothesis testing, I think it might be best to just generate bootstrap confidence intervals and call it a day. I'd rather be more explicit in my uncertainty and effect size than bank on a p-value that may or may not have a noteworthy effect. -->

```{r}
test_contrast_t <- function(tbl) {
  tests <- vector('list', length(forms))
  for (i in 1:length(forms)) {  # We do weird indexing to be able to still analyze when not all forms (1-7) are available
    S_match_summary <- tbl %>% filter(Experiment==forms[i] & Correct==T & Conn.Type=='Summary') %>% select(Inference)
    S_match_contrast <- tbl %>% filter(Experiment==forms[i] & Correct==T & Conn.Type=='Contrast') %>% select(Inference)
    test <- t.test(S_match_summary, S_match_contrast, var.equal=F)
    tests[[i]] <- test
  }
  return(tests)
}

ctlm1b <- test_contrast_t(lm1b)
ctgpt2 <- test_contrast_t(gpt2)
ctlm1b
ctgpt2
```
<!--
Onto the permutation test logic:

```{r}
diff_mean_stat <- function(data, labels, labelX, labelY) {
  x <- data[labels==labelX]
  y <- data[labels==labelY]
  return(mean(x) - mean(y))
}

permute_diff_mean <- function(data, labels, labelX, labelY) {
  return(diff_mean_stat(data, sample(labels, length(labels), replace=F), labelX, labelY))
}

test_contrast_permute <- function(tbl, iter=1e5) {
  tests <- vector('list', length(forms))
  labelX <- 'Summary'
  labelY <- 'Contrast'
  for (i in 1:length(forms)) {
    # Setup
    data <- tbl %>% filter(Experiment==forms[i] & Correct==T)
    labels <- data$Conn.Type
    data <- data$Inference
    t.obs <- diff_mean_stat(data, labels, labelX, labelY)
    # Generate one permutation and calculate the difference of means statistic 
    perm_dist <- replicate(iter, permute_diff_mean(data, labels, labelX, labelY))
    # Get p-value
    # p <- min(mean(t.obs > perm_dist), mean(t.obs < perm_dist)) * 2  # 2-sided
    p <- mean(perm_dist < t.obs)
    tests[[i]] <- list(p=p, DiffMean=t.obs, NullDistribution=perm_dist)
  }
  # Convert funky list into a nice tbl
  tests_tbl <- tbl_df(t(sapply(tests, c))) %>%
    mutate(p=as.numeric(p)) %>%
    mutate(DiffMean=as.numeric(DiffMean))
  return(tests_tbl)
}
```

```{r}
cplm1b <- test_contrast_permute(lm1b, iter=1e5)
cpgpt2 <- test_contrast_permute(gpt2, iter=1e5)
cplm1b
cpgpt2
```

Looking to a visual (*graph moved to lower notebook point*), it's certainly interesting, hm. I'm a little worried about the multiple-testing scenario we're getting close to. And some p-values above are interesting, but some are borderline. I think, perhaps, our best bet will be to do a bootstrap to construct some confidence intervals.
-->

```{r}
diff_mean_stat <- function(x, indices) {
 data <- x[indices,]
 return(mean(filter(data, Conn.Type=='Summary')$Inference) -
        mean(filter(data, Conn.Type=='Contrast')$Inference))
}

# Calculates simple bootstrap confidence intervals for a difference in means
# between [Matching/Summary] and [Matching/Contrast]
# with stratified sampling on connective type (but--this doesn't seem to have a huge impact in testing?)
diff_mean.CI <- function(tbl, iter=1e4) {
  tests <- array(dim=c(length(forms), 4), dimnames=list(NULL, c('Experiment', 'Diff.Mean', 'CI.lower', 'CI.upper')))
  for (i in 1:length(forms)) {
    # Setup
    data <- tbl %>%
      filter(Experiment==forms[i] & Correct==T) %>%
      mutate(strata=ifelse(Conn.Type=='Summary', 1, 2))
    # Do boostrap
    b <- boot(data, statistic=diff_mean_stat, R=iter, strata=data$strata)
    ci <- boot.ci(b, conf=0.95, type='basic')
    # Save statistic, basic confidence interval, and experiment no.
    tests[i, 'Experiment'] <- forms[i]
    tests[i, 'Diff.Mean'] <- b$t0
    tests[i, 'CI.lower'] <- ci$basic[4]
    tests[i, 'CI.upper'] <- ci$basic[5]
  }
  return(tbl_df(tests))
}
```

```{r}
lm1b_contrast_ci <- diff_mean.CI(lm1b, iter=3e4)
gpt2_contrast_ci <- diff_mean.CI(gpt2, iter=3e4)  # max(table(lm1b$Experiment)) = 28800

contrasts <- bind_rows(
  lm1b_contrast_ci %>% mutate(Model='LM1B'),
  gpt2_contrast_ci %>% mutate(Model='GPT2')
)

ggplot(contrasts, aes(x=Experiment, y=Diff.Mean, fill=Model)) +
  geom_bar(stat="identity", position="dodge") +
  # theme(legend.position="none") +
  geom_errorbar(aes(ymin=CI.lower, ymax=CI.upper), color='black', position='dodge') +
  scale_x_continuous(breaks=forms) +
  xlab("Contrasts, Diff Means")
```
 
These results don't bode very well---for LM1B in general, and for GPT-2 on Experiments 2, 6, and 7. Interesting.

# To be less surprised at correct-summ than incorrect-summ?

<!-- Overall limitation is that this definitely doesn't have predictive power -->