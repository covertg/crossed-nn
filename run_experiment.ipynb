{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossing import experiment\n",
    "n_forms = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer for GPT-2\n",
      "[experiment.py] Experiment 1\t [START] 06:20:35.005066\t [END] 06:30:28.662192\n",
      "[experiment.py] Experiment 2\t [START] 06:30:28.662307\t [END] 06:38:12.958766\n",
      "[experiment.py] Experiment 3\t [START] 06:38:12.958909\t [END] 07:25:20.244771\n",
      "[experiment.py] Experiment 4\t [START] 07:25:20.244869\t [END] 08:00:37.201313\n",
      "[experiment.py] Experiment 5\t [START] 08:00:37.201404\t [END] 08:35:04.235850\n",
      "[experiment.py] Experiment 6\t [START] 08:35:04.235951\t [END] 08:49:42.469379\n",
      "[experiment.py] Experiment 7\t [START] 08:49:42.469480\t [END] 09:07:03.211983\n"
     ]
    }
   ],
   "source": [
    "from crossing import gpt2\n",
    "model_fn, model_name = gpt2.get_model_fn(), 'gpt2'\n",
    "\n",
    "n_exps = range(0, 7)\n",
    "experiment.run_experiments('data/sents%d.tsv', 'results/'+model_name+'_%d.tsv', model_fn, n_exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lm_1b Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading vocabulary lm_1b/vocab-2016-09-10.txt\n",
      "Reading graph lm_1b/graph-2016-09-10.pbtxt\n",
      "Creating tensors.\n",
      "Loading, initializing weights lm_1b/ckpt-*\n",
      "Loaded model and vocab for lm_1b.\n",
      "[experiment.py] Experiment 1\t [START] 12:29:17.213485\n",
      "[experiment.py] Experiment 1\t [END] 22:37:38.323635\n",
      "[experiment.py] Experiment 2\t [START] 22:37:38.323706\n",
      "[experiment.py] Experiment 2\t [END] 09:30:03.497042\n",
      "[experiment.py] Experiment 3\t [START] 09:30:03.497115\n",
      "[experiment.py] Experiment 3\t [END] 08:06:57.185201\n",
      "[experiment.py] Experiment 4\t [START] 08:06:57.185272\n",
      "[experiment.py] Experiment 4\t [END] 01:57:51.920371\n",
      "[experiment.py] Experiment 5\t [START] 01:57:51.920458\n",
      "[experiment.py] Experiment 5\t [END] 19:49:12.361876\n",
      "[experiment.py] Experiment 6\t [START] 19:49:12.361958\n"
     ]
    }
   ],
   "source": [
    "from crossing import lm_1b\n",
    "model_fn, model_name = lm_1b.get_model_fn('lm_1b/graph-2016-09-10.pbtxt', 'lm_1b/ckpt-*', 'lm_1b/vocab-2016-09-10.txt'), 'lm1b'\n",
    "\n",
    "n_exps = range(0, 5) # Experiments run on desktop\n",
    "experiment.run_experiments('data/small_sents%d.tsv', 'results/'+model_name+'_%d.tsv', model_fn, n_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading vocabulary lm_1b/vocab-2016-09-10.txt\n",
      "Reading graph lm_1b/graph-2016-09-10.pbtxt\n",
      "Creating tensors.\n",
      "Loading, initializing weights lm_1b/ckpt-*\n",
      "Loaded model and vocab for lm_1b.\n",
      "[experiment.py] Experiment 6\t [START] 10:56:21.623330\n",
      "[experiment.py] Processed 100 lines \t 12:48:59.935280\n",
      "[experiment.py] Processed 200 lines \t 14:15:33.521672\n",
      "[experiment.py] Processed 300 lines \t 16:11:02.339217\n",
      "[experiment.py] Processed 400 lines \t 18:09:00.952399\n",
      "[experiment.py] Processed 500 lines \t 19:42:05.272747\n",
      "[experiment.py] Processed 600 lines \t 21:12:55.404877\n",
      "[experiment.py] Processed 700 lines \t 22:43:53.686479\n",
      "[experiment.py] Processed 800 lines \t 00:14:25.097630\n",
      "[experiment.py] Processed 900 lines \t 01:44:58.088525\n",
      "[experiment.py] Processed 1000 lines \t 03:14:40.014283\n",
      "[experiment.py] Processed 1100 lines \t 04:45:25.449408\n",
      "[experiment.py] Processed 1200 lines \t 06:15:38.811933\n",
      "[experiment.py] Processed 1300 lines \t 07:46:40.532834\n",
      "[experiment.py] Processed 1400 lines \t 09:17:26.548103\n",
      "[experiment.py] Processed 1500 lines \t 10:56:33.516807\n",
      "[experiment.py] Processed 1600 lines \t 12:57:44.099871\n",
      "[experiment.py] Experiment 6\t [END] 12:57:44.149473\n",
      "[experiment.py] Experiment 7\t [START] 12:57:44.149598\n",
      "[experiment.py] Processed 100 lines \t 14:36:19.139741\n",
      "[experiment.py] Processed 200 lines \t 16:09:53.549564\n",
      "[experiment.py] Processed 300 lines \t 17:47:54.700756\n",
      "[experiment.py] Processed 400 lines \t 19:23:21.857001\n",
      "[experiment.py] Processed 500 lines \t 20:57:39.096154\n",
      "[experiment.py] Processed 600 lines \t 22:33:34.075473\n",
      "[experiment.py] Processed 700 lines \t 00:25:36.600591\n",
      "[experiment.py] Processed 800 lines \t 02:28:22.208550\n",
      "[experiment.py] Processed 900 lines \t 04:16:30.545596\n",
      "[experiment.py] Processed 1000 lines \t 05:51:02.837248\n",
      "[experiment.py] Processed 1100 lines \t 07:25:24.093256\n",
      "[experiment.py] Processed 1200 lines \t 09:00:09.103831\n",
      "[experiment.py] Processed 1300 lines \t 10:38:26.527277\n",
      "[experiment.py] Processed 1400 lines \t 12:29:56.286879\n",
      "[experiment.py] Processed 1500 lines \t 14:10:32.451603\n",
      "[experiment.py] Processed 1600 lines \t 15:45:48.720664\n",
      "[experiment.py] Experiment 7\t [END] 15:45:48.796647\n"
     ]
    }
   ],
   "source": [
    "from crossing import lm_1b\n",
    "model_fn, model_name = lm_1b.get_model_fn('lm_1b/graph-2016-09-10.pbtxt', 'lm_1b/ckpt-*', 'lm_1b/vocab-2016-09-10.txt'), 'lm1b'\n",
    "\n",
    "n_exps = range(5,7)  # Experiments run on laptop\n",
    "experiment.run_experiments('data/small_sents%d.tsv', 'results/'+model_name+'_%d.tsv', model_fn, n_exps, silent=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
